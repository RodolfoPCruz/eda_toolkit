{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "db82e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn import set_config\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "503ab793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn will return a pandas dataframe insted of a numpy array\n",
    "set_config(transform_output=\"pandas\")\n",
    "random_state = 42\n",
    "sns.set_style(\"darkgrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26953fb",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "849ffc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba       = pd.read_csv('../data/nba/nba_salaries.csv')\n",
    "insurance = pd.read_csv('../data/insurance/insurance.csv')\n",
    "airline   = pd.read_csv('../data/airline/train.csv')\n",
    "airbnb    = pd.read_csv('../data/airbnb/listings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7ac45f",
   "metadata": {},
   "source": [
    "# Managing outliers one feature at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "be8b595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_outliers(df: pd.DataFrame, features_list: list = None, \n",
    "                   outlier_treatment: str = None, output_column: str = None, \n",
    "                   skew_thresold: float = 1):\n",
    "    '''\n",
    "    Detect and treat outliers. Two criteria will be used to detect outliers:\n",
    "        - empirical rule to detect outliers in normal distributions;\n",
    "        - Tukey rule to detect outliers in distributions that are not normal.\n",
    "    A distribution will be considered normal when its skewness is between -1 * skew_thresold and skew_thresold.\n",
    "    One of the two criteria will be used to calculate max_thresh and min_thresh. Values above max_thresh and \n",
    "    values below min_thresh are considered outliers.\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The dataframe to be cleaned.\n",
    "    features_list : list, optional\n",
    "        The list of features to be cleaned, by default, is None. If None, the function will clean all features.\n",
    "    outlier_treatment : str, optional\n",
    "        The treatment to be applied to outliers, by default, None. Can be 'remove'; 'replace'; 'impute', or None:\n",
    "            - remove: rows containing outliers are completely removed;\n",
    "            - replace: outliers above max_thresh and outliers below min_thresh are replaced by max_thresh and min_thresh, respectively;\n",
    "            - impute: outliers above max_thresh and outliers below min_thresh are imputed using the Iterative imputer method form scikit-learn;\n",
    "            - None: no treatment is applied to outliers.\n",
    "    output_column : str, optional\n",
    "        The name of the output column, by default is None. The output column will not be used when calculating the values to be imputed. \n",
    "        If None, it is considered that the output column is not in df.\n",
    "    skew_thresold : float, optional\n",
    "        The skewness threshold for a distribution to be considered normal, by default 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The cleaned dataframe.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    df = df.copy()\n",
    "    \n",
    "    if features_list is None:\n",
    "        features_list = df.columns\n",
    "\n",
    "    if outlier_treatment not in ['remove', 'replace', 'impute', None]:\n",
    "        print(f'The outlier_treatment {outlier_treatment} is not valid. No treatment will be applied to outliers.')\n",
    "        outlier_treatment = None\n",
    "\n",
    "    #remove output_column from features_list. The output column can not be used when calculating the values to be imputed\n",
    "    if output_column is not None and output_column in features_list:\n",
    "        features_list_impute_method = features_list.copy()\n",
    "        features_list_impute_method.remove(output_column)\n",
    "    else:\n",
    "        features_list_impute_method = features_list\n",
    " \n",
    "\n",
    "    for feature in features_list:\n",
    "        #test whether the feature is in the dataframe\n",
    "        if feature in df.columns:\n",
    "            #only numeric columns will be cleaned\n",
    "            if pd.api.types.is_numeric_dtype(df[feature]):\n",
    "\n",
    "                #test whether the feature has only one unique value\n",
    "                if df[feature].nunique() == 1:\n",
    "                    print(f'The feature {feature} has only one unique value and was therefore ignored.')\n",
    "\n",
    "                #test whether the feature is binary (only true or false values)\n",
    "                elif (set(df[feature].dropna().unique()).issubset({0, 1}) or\n",
    "                        set(df[feature].dropna().unique()).issubset({True, False})):\n",
    "                    print(f'The feature {feature} is binary (only true or false values) ans was therefore ignored.')\n",
    "\n",
    "                # look for outliers\n",
    "                else:\n",
    "                    #test whether the distribution is normal\n",
    "                    skew = df[feature].skew()\n",
    "                    if skew > -1 * skew_thresold and skew < skew_thresold:\n",
    "                        #empirical rule to detect outliers in normal distributions\n",
    "                        min_thresh = df[feature].mean() - 3 * df[feature].std()\n",
    "                        max_thresh = df[feature].mean() + 3 * df[feature].std()\n",
    "                    else: \n",
    "                        #apply the Tukey rule to detect outliers in distributions that are not normal\n",
    "                        q1 = df[feature].quantile(0.25)\n",
    "                        q3 = df[feature].quantile(0.75)\n",
    "                        iqr = q3 - q1 #interquartile range\n",
    "                        max_thresh = q3 + 1.5 * (iqr) \n",
    "                        min_thresh = q1 - 1.5 * (iqr)\n",
    "\n",
    "                    #values above max_thresh and values below min_thresh are considered outliers\n",
    "                    count_max_outlier = len(df.loc[df[feature] > max_thresh])\n",
    "                    count_min_outlier = len(df.loc[df[feature] < min_thresh])\n",
    "                    print(f'The feature {feature} has {count_max_outlier} values above {max_thresh}')\n",
    "                    print(f'The feature {feature} has {count_min_outlier} values below {min_thresh}')\n",
    "\n",
    "                    #deal with outliers\n",
    "                    if count_max_outlier > 0 and outlier_treatment != 'impute':\n",
    "                        if outlier_treatment == 'remove':\n",
    "                            print(f'{count_max_outlier} values above {max_thresh} were removed in feature {feature}')  \n",
    "                            df = df[df[feature] < max_thresh]\n",
    "\n",
    "                        elif outlier_treatment == 'replace':\n",
    "                            if not pd.api.types.is_float_dtype(df[feature]):\n",
    "                                df[feature] = df[feature].astype(float) #convert to float\n",
    "                            print(f'{count_max_outlier} values above {max_thresh} were replaced in feature {feature}')  \n",
    "                            df.loc[df[feature] > max_thresh, feature] = max_thresh\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    if count_min_outlier > 0 and outlier_treatment != 'impute':\n",
    "                        if outlier_treatment == 'remove':\n",
    "                            print(f'{count_min_outlier} values below {min_thresh} were removed in feature {feature}')  \n",
    "                            df = df[df[feature] > min_thresh]\n",
    "                        elif outlier_treatment == 'replace':\n",
    "                            if not pd.api.types.is_float_dtype(df[feature]):\n",
    "                                df[feature] = df[feature].astype(float) #convert to float\n",
    "                            print(f'{count_min_outlier} values below {min_thresh} were replaced in feature {feature}')  \n",
    "                            df.loc[df[feature] < min_thresh, feature] = min_thresh\n",
    "                        else:\n",
    "                            continue\n",
    "                    #impute method\n",
    "                    has_outliers = count_max_outlier > 0 or count_min_outlier > 0\n",
    "                    if outlier_treatment == 'impute' and has_outliers:\n",
    "                        #numeric_columns = df.select_dtypes(include = 'number').columns.to_list()\n",
    "                        df_temp = df.copy()\n",
    "                        df_temp.loc[df_temp[feature] > max_thresh, feature] = np.nan\n",
    "                        df_temp.loc[df_temp[feature] < min_thresh, feature] = np.nan\n",
    "                        #if output_column is not None and output_column in numeric_columns:\n",
    "                        #    numeric_columns.remove(output_column)\n",
    "                        df_temp = pd.get_dummies(df_temp, drop_first = True)\n",
    "                        imputer = IterativeImputer(max_iter=10, random_state = random_state)\n",
    "                        df_temp = imputer.fit_transform(df_temp)\n",
    "                        df[feature] = df_temp[feature]\n",
    "\n",
    "            else:\n",
    "                print(f'The feature {feature} is not numeric and was therefore ignored')\n",
    "\n",
    "        else:\n",
    "            print(f'A {feature} não foi encontrada no dataframe e foi ignorada')\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7947fbb2",
   "metadata": {},
   "source": [
    "# Managing Outliers all features at once using DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "ceb2ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_eps_dbscan(df: pd.DataFrame, distance_metric: str = 'manhattan', \n",
    "                      min_samples: int = 5, eps_step : float = 0.01, desired_percentage_outliers : float = 0.02 ):\n",
    "\n",
    "    '''\n",
    "    Function to look for the best eps value for DBSCAN. Different eps values are tested to determine the proportion of outliers they produce.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The dataframe to be cleaned.\n",
    "    distance_metric : str, optional\n",
    "        The distance metric to be used in DBSCAN. The default is 'manhattan'. This metric is used by \n",
    "        DBSCAN to calculate the distance between points.\n",
    "    min_samples : int, optional          \n",
    "        The minimum samples to be used in DBSCAN. The default is 5. It is the maximum distance between \n",
    "        two samples for one to be considered as in the neighborhood of the other.The larger the eps value, \n",
    "        the fewer outliers there will be.\n",
    "    eps_step : float, optional\n",
    "        The step size to be used in the eps search. The default is 0.01. The step_size is used to increment\n",
    "        the eps value.    \n",
    "    desired_percentage_outliers : float, optional\n",
    "        The desired percentage of outliers to be removed. The default is 0.02. The function will return the eps value\n",
    "        that produces the desired percentage of outliers.\n",
    "        \n",
    "    Returns\n",
    "    ------- \n",
    "    eps_percentage_outliers : pandas DataFrame\n",
    "        A dataframe with the eps values and the percentage of outliers they produce.\n",
    "    desired_percentage_outliers : float\n",
    "        The eps value that produces the desired percentage of outliers.              \n",
    "    '''\n",
    "    \n",
    "    df_temp = df.copy()\n",
    "\n",
    "    #remove NaN\n",
    "    initial_number_of_columns = df.shape[1]\n",
    "    initial_number_of_rows = df.shape[0]\n",
    "    initial_number_of_nan = df_temp.isna().sum().sum()  \n",
    "    print(f'Number of missing values: {initial_number_of_nan}')\n",
    "\n",
    "    df_temp = df_temp.dropna(axis = 'columns', how = 'all')\n",
    "    df_temp = df_temp.dropna(axis = 'rows', how = 'any')\n",
    "\n",
    "    final_number_of_columns = df_temp.shape[1]\n",
    "    final_number_of_rows = df_temp.shape[0]\n",
    "\n",
    "    print(f'Number of columns removed: {initial_number_of_columns - final_number_of_columns}')\n",
    "    print(f'Number of rows removed: {initial_number_of_rows - final_number_of_rows}')\n",
    "    print(f'Number of missing values after cleaning: {df_temp.isna().sum().sum()}')\n",
    "\n",
    "    #Transform categorical variables \n",
    "    df_temp = pd.get_dummies(df_temp, drop_first = True)\n",
    "\n",
    "    #Min max Scaler\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    df_temp = min_max_scaler.fit_transform(df_temp)\n",
    "\n",
    "    #test DBSCAN using different eps to determine the number of outliers they produce\n",
    "    eps_percentage_outliers = {'eps' : [],\n",
    "                               'percentage_outliers(%)' : []}\n",
    "    num_outliers = len(df_temp)\n",
    "    eps = 0\n",
    "    while num_outliers > 0:\n",
    "        eps += eps_step\n",
    "        dba = DBSCAN(eps = eps, min_samples = min_samples, metric=distance_metric, n_jobs = -1)\n",
    "        dba.fit(df_temp)\n",
    "        labels = dba.labels_\n",
    "        num_outliers = np.count_nonzero(labels == -1)\n",
    "        eps_percentage_outliers['eps'].append(eps)\n",
    "        eps_percentage_outliers['percentage_outliers(%)'].append(round(100 * num_outliers/len(df_temp), 2))\n",
    "    eps_percentage_outliers = pd.DataFrame(eps_percentage_outliers) \n",
    "    \n",
    "    #look for the eps value that produces the desired percentage of outliers\n",
    "    eps_percentage_outliers['diff'] = eps_percentage_outliers['percentage_outliers(%)'] - 100 * desired_percentage_outliers\n",
    "    eps_percentage_outliers['diff'] = np.abs(eps_percentage_outliers['diff'])\n",
    "    idx_min = eps_percentage_outliers['diff'].idxmin()\n",
    "    desired_eps = round(eps_percentage_outliers.loc[idx_min, 'eps'], 2)\n",
    "    percentage_outliers = eps_percentage_outliers.loc[idx_min, 'percentage_outliers(%)']\n",
    "\n",
    "    #plot a lineplot with the eps values and the percentage of outliers they produce\n",
    "    ax = sns.lineplot(data = eps_percentage_outliers , x = 'eps', y = 'percentage_outliers(%)')\n",
    "    plt.scatter(desired_eps, percentage_outliers)\n",
    "    text = f'eps = {desired_eps} \\n'\n",
    "    text += f'percentage_outliers(%) = {percentage_outliers}'\n",
    "    plt.annotate(text, \n",
    "                xy=(desired_eps, percentage_outliers), \n",
    "                xytext=(desired_eps + 0.1, percentage_outliers + 0.1),\n",
    "                arrowprops=dict(arrowstyle='->', color='gray'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    return eps_percentage_outliers, desired_eps\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6660aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_outliers_using_DBscan(df: pd.DataFrame, eps: float, distance_metric: str = 'manhattan', \n",
    "                                min_samples: int = 5):\n",
    "    \n",
    "    '''\n",
    "    Funcion to remove outliers using DBSCAN\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas DataFrame\n",
    "        The dataframe to be cleaned.\n",
    "    eps : float\n",
    "        The eps value to be used in DBSCAN. This value is the maximum distance between two samples for one to be \n",
    "        considered as in the neighborhood of the other.\n",
    "    distance_metric : str, optional          \n",
    "        The metric to be used in DBSCAN. The default is 'manhattan'. It is the metric used to calculate the distance between points.\n",
    "    min_samples : int, optional          \n",
    "        The minimum samples to be used in DBSCAN. The default is 5. It is the maximum distance between \n",
    "        two samples for one to be considered as in the neighborhood of the other.The larger the eps value, \n",
    "        the fewer outliers there will be.\n",
    "\n",
    "    Returns\n",
    "    ------- \n",
    "    df : pandas DataFrame\n",
    "        The cleaned dataframe.\n",
    "    '''\n",
    "\n",
    "    df = pd.copy(df)\n",
    "    df_temp = df.copy()\n",
    "\n",
    "    #remove NaN\n",
    "    initial_number_of_columns = df.shape[1]\n",
    "    initial_number_of_rows = df.shape[0]\n",
    "    initial_number_of_nan = df_temp.isna().sum().sum()  \n",
    "    print(f'Number of missing values: {initial_number_of_nan}')\n",
    "\n",
    "    df_temp = df_temp.dropna(axis = 'columns', how = 'all')\n",
    "    df_temp = df_temp.dropna(axis = 'rows', how = 'any')\n",
    "\n",
    "    final_number_of_columns = df_temp.shape[1]\n",
    "    rows_no_nan = df_temp.shape[0]\n",
    "\n",
    "    print(f'Number of columns removed: {initial_number_of_columns - final_number_of_columns}')\n",
    "    print(f'Number of rows removed: {initial_number_of_rows - rows_no_nan}')\n",
    "    print(f'Number of missing values after cleaning: {df_temp.isna().sum().sum()}')\n",
    "\n",
    "    #Transform categorical variables \n",
    "    df_temp = pd.get_dummies(df_temp, drop_first = True)\n",
    "\n",
    "    #Min max Scaler\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    df_temp = min_max_scaler.fit_transform(df_temp)\n",
    "\n",
    "    #DBSCAN to look for outliers\n",
    "    dba = DBSCAN(eps = eps, min_samples = min_samples, metric=distance_metric, n_jobs = -1)\n",
    "    dba.fit(df_temp)\n",
    "    labels = dba.labels_\n",
    "    df_temp['labels'] = labels\n",
    "    number_outliers = np.count_nonzero(labels == -1)\n",
    "    print(f'Number of outliers: {number_outliers}')\n",
    "\n",
    "    #remove outliers\n",
    "    index_outliers = df_temp[df_temp['labels'] == -1].index\n",
    "    df = df.drop(index_outliers)\n",
    "\n",
    "    print(f'Percentage of rows removed: {(number_outliers / rows_no_nan) * 100}%')    \n",
    "\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automacao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
